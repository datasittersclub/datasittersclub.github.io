Traceback (most recent call last):
  File "/Users/qad/anaconda3/envs/dsc/lib/python3.7/site-packages/jupyter_cache/executors/utils.py", line 64, in single_nb_execution
    **kwargs,
  File "/Users/qad/anaconda3/envs/dsc/lib/python3.7/site-packages/nbclient/client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/Users/qad/anaconda3/envs/dsc/lib/python3.7/site-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/Users/qad/anaconda3/envs/dsc/lib/python3.7/site-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "/Users/qad/anaconda3/envs/dsc/lib/python3.7/asyncio/base_events.py", line 587, in run_until_complete
    return future.result()
  File "/Users/qad/anaconda3/envs/dsc/lib/python3.7/site-packages/nbclient/client.py", line 664, in async_execute
    cell, index, execution_count=self.code_cells_executed + 1
  File "/Users/qad/anaconda3/envs/dsc/lib/python3.7/site-packages/nbclient/client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/qad/anaconda3/envs/dsc/lib/python3.7/site-packages/nbclient/client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
from sklearn.feature_extraction.text import TfidfVectorizer

# Use the glob library to create a list of file names, sorted alphabetically
# Alphabetical sorting will get us the books in numerical order
filenames = sorted(glob.glob("*.txt"))
# Parse those filenames to create a list of file keys (ID numbers)
# You'll use these later on.
filekeys = [f.split('/')[-1].split('.')[0] for f in filenames]

# Create a CountVectorizer instance with the parameters you need
freqvectorizer = TfidfVectorizer(input="filename", stop_words=None, use_idf=False, norm='l1', max_features=1000)
# Run the vectorizer on your list of filenames to create your wordcounts
# Use the toarray() function so that SciPy will accept the results
wordfreqs = freqvectorizer.fit_transform(filenames).toarray()
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m/var/folders/3r/55b5kjpd4s14_tg80r24vs7r0000gq/T/ipykernel_18885/1962200339.py[0m in [0;36m<module>[0;34m[0m
[1;32m     12[0m [0;31m# Run the vectorizer on your list of filenames to create your wordcounts[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[1;32m     13[0m [0;31m# Use the toarray() function so that SciPy will accept the results[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 14[0;31m [0mwordfreqs[0m [0;34m=[0m [0mfreqvectorizer[0m[0;34m.[0m[0mfit_transform[0m[0;34m([0m[0mfilenames[0m[0;34m)[0m[0;34m.[0m[0mtoarray[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m
[0;32m~/anaconda3/envs/dsc/lib/python3.7/site-packages/sklearn/feature_extraction/text.py[0m in [0;36mfit_transform[0;34m(self, raw_documents, y)[0m
[1;32m   1857[0m         """
[1;32m   1858[0m         [0mself[0m[0;34m.[0m[0m_check_params[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1859[0;31m         [0mX[0m [0;34m=[0m [0msuper[0m[0;34m([0m[0;34m)[0m[0;34m.[0m[0mfit_transform[0m[0;34m([0m[0mraw_documents[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m   1860[0m         [0mself[0m[0;34m.[0m[0m_tfidf[0m[0;34m.[0m[0mfit[0m[0;34m([0m[0mX[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1861[0m         [0;31m# X is already a transformed view of raw_documents so[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;32m~/anaconda3/envs/dsc/lib/python3.7/site-packages/sklearn/feature_extraction/text.py[0m in [0;36mfit_transform[0;34m(self, raw_documents, y)[0m
[1;32m   1218[0m [0;34m[0m[0m
[1;32m   1219[0m         vocabulary, X = self._count_vocab(raw_documents,
[0;32m-> 1220[0;31m                                           self.fixed_vocabulary_)
[0m[1;32m   1221[0m [0;34m[0m[0m
[1;32m   1222[0m         [0;32mif[0m [0mself[0m[0;34m.[0m[0mbinary[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;32m~/anaconda3/envs/dsc/lib/python3.7/site-packages/sklearn/feature_extraction/text.py[0m in [0;36m_count_vocab[0;34m(self, raw_documents, fixed_vocab)[0m
[1;32m   1148[0m             [0mvocabulary[0m [0;34m=[0m [0mdict[0m[0;34m([0m[0mvocabulary[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m   1149[0m             [0;32mif[0m [0;32mnot[0m [0mvocabulary[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1150[0;31m                 raise ValueError("empty vocabulary; perhaps the documents only"
[0m[1;32m   1151[0m                                  " contain stop words")
[1;32m   1152[0m [0;34m[0m[0m

[0;31mValueError[0m: empty vocabulary; perhaps the documents only contain stop words
ValueError: empty vocabulary; perhaps the documents only contain stop words

