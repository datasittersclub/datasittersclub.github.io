
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>DSC Multilingual Mystery 4: Isabelle and the Missing Spaghetti-O’s &#8212; Data-Sitters Club</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/style.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/DSClogo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data-Sitters Club</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter-2.html">
   About the DSC
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="books.html">
   DSC Books
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Main series
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dsc1.html">
   DSC 1: Quinn's Great Idea
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dsc2.html">
   DSC 2: Katia and the Phantom Corpus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dsc3.html">
   DSC 3: The Truth About DH Collaboration (and Textual Variants!)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dsc4.html">
   DSC 4: AntConc Saves the Day
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dsc5.html">
   DSC 5: The DSC and the Impossible TEI Quandaries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dsc6.html">
   DSC 6: Voyant's Big Day
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dsc7.html">
   DSC 7: The DSC and Mean Copyright Law
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dsc8.html">
   DSC 8: Text-Comparison-Algorithm-Crazy Quinn
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dsc9.html">
   DSC 9: The Ghost in Anouk's Laptop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dsc10.html">
   DSC 10: Heather Loves PCA
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Multilingual mysteries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dscm1.html">
   DSCM 1: Lee and the Missing Metadata
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dscm2.html">
   DSCM 2: Beware, Lee and Quinn!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dscm3.html">
   DSCM 3: Quinn and Lee Clean Up Ghost Cat Data-Hairballs
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Fun
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="covid19.html">
   Important Public Health Messages from the Data-Sitters Club about COVID-19
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dhawards2019.html">
   DH Awards 2019 Ad Campaign
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/dscm4-prodigy.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/dscm4-prodigy.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quinn">
   Quinn
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pandemic-time-warp">
     Pandemic Time Warp
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#meet-isabelle">
     Meet Isabelle
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#isabelle">
   Isabelle
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lee">
   Lee
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Quinn
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#foodbase-off-base">
     FoodBase, off base
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#edible-genealogies">
     Edible Genealogies
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#install-modules">
       Install modules
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#choose-the-directory">
       Choose the directory
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#find-the-nouns">
       Find the nouns
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#data-cleaning">
       Data cleaning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#finding-unique-words">
       Finding unique words
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#running-words-through-wordnet">
       Running words through WordNet
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#extracting-the-food-words">
       Extracting the food words
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-train-an-ner-model">
     Why Train an NER Model?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-not-train-an-ner-model">
     Why not train an NER model?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#installing-prodigy">
       Installing Prodigy
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#preparing-a-sample-for-annotation">
       Preparing a Sample for Annotation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#annotating-with-prodigy">
       Annotating with Prodigy
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#consistency-consistency-consistency">
       Consistency, Consistency, Consistency
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model-training">
       Model Training
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-alignment">
     Text alignment
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#importing-modules">
       Importing modules
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#choosing-the-directory">
       Choosing the directory
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#splitting-text-into-sentences">
       Splitting text into sentences
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#machine-translation-limits">
       Machine translation limits
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bleualigning-all-the-things">
       Bleualigning All The Things
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparing-just-the-food">
     Comparing just the food
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#specifying-the-directory-and-language">
       Specifying the directory and language
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#specifying-the-word-list">
       Specifying the word list
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#identifying-sentence-pairs-with-food-words">
       Identifying sentence pairs with food words
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Isabelle
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   Quinn
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   Lee
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p><img alt="DSC logo" src="_images/DSCLogo.png" /></p>
<div class="section" id="dsc-multilingual-mystery-4-isabelle-and-the-missing-spaghetti-o-s">
<h1>DSC Multilingual Mystery 4: Isabelle and the Missing Spaghetti-O’s<a class="headerlink" href="#dsc-multilingual-mystery-4-isabelle-and-the-missing-spaghetti-o-s" title="Permalink to this headline">¶</a></h1>
<div style='float: right; width: 200px;margin-left: 7px;margin-top: 0px;'>
<img src='_static/images/bookcovers/dscm4_cover.jpg' alt='DSC M3 book cover' />
</div>
<p>By Lee Skallerup-Bessette, Quinn Dombrowski, and Isabelle Gribomont</p>
<p>July 22, 2021</p>
<div class="section" id="quinn">
<h2>Quinn<a class="headerlink" href="#quinn" title="Permalink to this headline">¶</a></h2>
<div class="section" id="pandemic-time-warp">
<h3>Pandemic Time Warp<a class="headerlink" href="#pandemic-time-warp" title="Permalink to this headline">¶</a></h3>
<p>It’s been over a year since Lee and I <a class="reference external" href="https://datasittersclub.github.io/site/dscm3.html">cleaned up ghost cat data-hairballs</a> with webscraper.io and OpenRefine.</p>
<p>In retrospect, it’s easy to recognize that book for what it was: 10,000 words of early-pandemic stress energy. While everyone else was buying up all the yeast and flour and baking bread with it, Lee and I were scraping a fan wiki and making the ultimate metadata sheet for our corpus.</p>
<p>We raced forward into April 2020, adopting translation corpus-building as a pandemic hobby. Lee arranged for a used bookseller in Quebec to ship every one of their <em>Les Baby-Sitters</em> books to my house. Cut off from the flatbed scanner I’d used at work due to the lockdown, I inquired if my department could buy me a document camera / scanner that I’d seen on an Instagram ad, of all places. The <a class="reference external" href="https://shop.czur.com/products/et16plus">Czur ET 16 plus</a> showed up on my 35th birthday and was the best present I could imagine; many thanks to Cécile Alduy, my department chair, for making it happen. (Her daughters are fans of the French graphic novels.)</p>
<p>But the pandemic wore on and on. Some days scanning and then proofreading the OCR seemed attainable. Other days, I shuddered at the prospect of fixing apostrophes and characters with accents on the clunky Windows laptop running ABBYY FineReader. Scanning and OCRing the whole original English BSC corpus was one thing in fall 2019 (when, in retrospect, all things felt attainable); slogging through so many French translations during 2020 was another matter.</p>
<p>At the beginning of the fall, Lee and I took a look at the French translations of the graphic novels that we’d explored in <a class="reference external" href="https://datasittersclub.github.io/site/dsc5.html">DSC #5: The DSC and the Impossible TEI Quandaries</a> for a talk called “<a class="reference external" href="https://docs.google.com/presentation/d/15SZjrQis3Xd3N3AEtaWuatIR-PeM6cAnD1H5oBbd_NQ/edit#slide=id.p">Layered Adaptation and Warped Nostalgia: Francophone Translations of the </a></p>
<p><a class="reference external" href="https://docs.google.com/presentation/d/15SZjrQis3Xd3N3AEtaWuatIR-PeM6cAnD1H5oBbd_NQ/edit#slide=id.p">Baby-Sitters Club Graphic Novels</a>” at virtual <a class="reference external" href="https://digital-frontiers.org/satellite-events/flyover/">Flyover Comics</a>. We used some of the methods from <a class="reference external" href="https://datasittersclub.github.io/site/dsc8.html">DSC #9: Text-Comparison-Algorithm-Crazy Quinn</a> to compare the graphic novels vs. the books they were based on, in English and French. (All made possible thanks to Katia, who left her self-isolation for the first time in four months to retrieve an errant package of Quebecois graphic novels, and her husband John, whom she enlisted to scan the graphic novels.)</p>
<p>But we didn’t write a multilingual mystery. Lee and I exchanged hundreds of text messages, but one thing we never confronted was the fact that we both were just stuck, the way Anouk and I got stuck on <a class="reference external" href="https://datasittersclub.github.io/site/dsc9.html">DSC #9: The Ghost in Anouk’s Laptop</a> around the same time. And it took a new addition to the DSC Multilingual Mystery team to get us out of our rut.</p>
</div>
<div class="section" id="meet-isabelle">
<h3>Meet Isabelle<a class="headerlink" href="#meet-isabelle" title="Permalink to this headline">¶</a></h3>
<p>Isabelle Gribomont has long, untidy, light-brown hair and green-ish eyes. She aspires to Claudia’s fashion sense but is most often found in lounge clothes and sporting a ginormous top-knot.</p>
<p>But, of course, that’s not how she looked when we met her. She was just another a black Zoom square, like most of the audience at our talk at the University of British Columbia on October 28, 2020. Lee was talking about how much work went into localizing the Belgian translations, but it was hard for us to fully appreciate it, not being familiar with Belgian culture.</p>
<p>After the talk, Isabelle tweeted:</p>
<p><img alt="Isabelle's tweet" src="_images/dscm4_isabelle_tweets.png" /></p>
<p>I nearly fell off my chair. I DMed her right away:</p>
<p><img alt="DMs with Isabelle" src="_images/dscm4_isabelle_dm.png" /></p>
</div>
</div>
<div class="section" id="isabelle">
<h2>Isabelle<a class="headerlink" href="#isabelle" title="Permalink to this headline">¶</a></h2>
<p>Before attending the UBC talk, I had been following the Data-Sitters Club from afar for a while and read some of the books they had published. Although I’m ashamed to say I had never read the Baby-Sitters Club series, I thought the project was pure genius. I was a <em>very</em> avid reader as a kid and have strong feelings about many children/YA book series to this day. As a more or less self-taught DH scholar, accessible and beginner-friendly DH tutorials also have a special place in my heart. Needless to say, I was very enthused by this cool project which was publishing down-to-earth DH experiments and giving ‘girly’ literature the attention it deserves.
The UBC talk was around 7pm in my timezone and made for a perfect lockdown evening entertainment. I clearly remember watching the talk on my tablet, slumped on the couch while sipping some herbal tea. When Lee mentioned the Belgian translations, I got very excited and a little surprised, since I always assumed French translations of children’s books were published in France. As someone who learned about the French currency, school system, food vocabulary, and more generally the minor differences between French and Belgian language varieties through children’s books, I immediately wondered if the translation was in fact recognisably set in Belgium and using Belgian language. Although I’m usually the lurking type on Twitter, I decided to tweet about the talk and mentioned my interest for the Belgian translations, secretly wishing I could get involved in some way. I couldn’t believe my luck when Quinn sent me a DM and I immediately agreed to contribute. Quinn shared the books and their translations with me and we had our first meeting with Lee soon after.</p>
<p>During that initial meeting, we discussed whatever struck us as interesting or weird in the Belgian and Quebecois translations. The first thing I personally noticed in the Belgian translations is that they were actually set in France, just like all my childhood books. Not only was the language resolutely French (e.g. ‘ninety’ was ‘quatre-vingt-dix’ instead of the Belgian ‘nonante’), but cultural references such as the currency and the school systems were French as well. This is not surprising considering that (today) there are 4.5 billion French speakers in Belgium versus 64 billion in France. Eighties and nineties Belgian kids grew up watching French television and reading French books. I was hoping that the Baby-Sitters Club could be a rare example of a reverse dynamic, but it makes sense that the publishing house, although located in Belgium, would appeal to the whole European Francophone market. That meant that my Belgian-ness was not as crucial after all, but Francophone Belgians are exposed enough to the largely overlapping French culture that it didn’t matter too much.</p>
<p>The second obvious discovery was that the localization was really thorough - every character name was Frenchified, New York became Paris, bees turned into flies (a more iconic classroom pest), and school meals were overhauled to resemble French lunches. The US American experience portrayed in the series was not considered an asset, but rather something to be camouflaged as convincingly as possible.</p>
</div>
<div class="section" id="lee">
<h2>Lee<a class="headerlink" href="#lee" title="Permalink to this headline">¶</a></h2>
<p>As a Canadian, I was always struck by the brands that were name-dropped in the Baby-Sitters Club books. In the 1980s, before you could buy anything and everything you could ever want off of the Internet, we had a much different brand-named food landscape in particular than our neighbors to the south. I still remember going to Florida as a kid for vacation and marveling at the sheer number and variety of (sugary) cereals we had long heard of while watching Saturday morning cartoons, but never were able to procure at our local grocery stores. And while we could get M&amp;Ms in Canada, we largely didn’t because we were loyal to our Cadbury version - Smarties.</p>
<p>And yes, Smarties down here in the States are those small, round sugar candies, but in Canada we call those Rockets, while Smarties for us were like M&amp;Ms.</p>
<p>When I was first reading the books with an eye to how they were translated, I was interested in more generally how the brands would be translated, but not just food brands - the toys, the board games, the books, etc. Would they be localized to, in particular, Quebec, remain Americanized, or split the difference? I wasn’t particularly focused on food at that point, just wanting to see, other than location, if and how Quebec would be represented through these brands. The brands did represent to me a kind of Americanness, and I had also grown up in Quebec to know what would have been an equivalent to that to give the books a more Québécois flavor.</p>
<p>But of course, unless I could train some model on “Americanness” and “Québécois” it would take a lot of really long close readings of all the things, which kinda defeats the purpose of this being a DH project. And it didn’t take into consideration either of the other two translations in terms of how they were or were not representing either Americanness vis-a-vis the target culture they were being translated into.</p>
<p>I shouldn’t be surprised that my default would be, hey, let’s just literally line up the books side by side (by side by side) and compare, since that’s what I did for my dissertation, but it was with poetry, so a bit more reasonable. We needed a different way to access the texts through a more sustainable approach, and focusing just on food was a revelation. Food, with or without the brand names attached to it, reflects a great deal about culture and class, and especially in the Baby-Sitters Club books. We may be initially struck by the sheer number of brand-name candies Claudia hides in her bedroom, but if you look beyond that, you find a what’s-what of 1980s suburban eating and cooking habits.</p>
<p>That would tell us way more than just focusing on brands. Thank goodness the Belgians made a really…interesting translation choice as Isabelle pointed it out. But more of that in a moment.</p>
</div>
<div class="section" id="id1">
<h2>Quinn<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class="section" id="foodbase-off-base">
<h3>FoodBase, off base<a class="headerlink" href="#foodbase-off-base" title="Permalink to this headline">¶</a></h3>
<p>When Isabelle, Lee, and I met a couple weeks after our chance encounter on Twitter, we hit it off immediately. It didn’t take long for us to agree that we would first investigate food across the translations. Because we had been focusing on brands, Isabelle had been focusing on that as well in her initial readings of the books. And then came the revelation that changed our approach: the Belgium translator had translated the 1980s staple canned pasta “Spagetti-Os” as the decadent veal dish Osso Buco. It was just so nonsensical (like, seriously, all they have in common is some form of tomato) that we started to think that food might be a great way to reframe our approach to the translations. We figured there must already be some resources for finding references to food in text – at least for English. It just felt like the kind of thing that data science people would do.</p>
<p>I started Googling, and was quickly rewarded with an open-access article from 2019: “<a class="reference external" href="https://academic.oup.com/database/article/doi/10.1093/database/baz121/5611291">FoodBase corpus: a new resource of annotated food entities</a>”, by Gorjan Popovski, Barbara Koroušić Seljak, and Tome Eftimov of Institute “Jozef Stefan’’ in Ljubljana, Slovenia. What I wanted was something I could just use with Python, out-of-the-box. But CS stuff almost never works that way, and for good reason: a project like FoodBase is useful for more kinds of projects– both applied projects that are actually looking to identify food, and computational projects that involve developing new algorithms using different kinds of test data– if it’s just annotated data. What’s more, its creators can actually be done with it and move on with their lives. This is not the case if they built a tool, where they would be stuck handling (or ignoring) error reports, technical components that require updating, people who fail to read the documentation, and all sorts of issues. This sort of conundrum happens a lot in DH, too: while there’s more demand from the field for ready-to-use tools that “just work”, that doesn’t translate into any more time, funding, or staffing resources to deal with the support demands than we see in computer science.</p>
<p>There’s another reason, though, why it turned out to be a good thing that FoodBase was just a corpus and not a tool: seeing the actual annotated texts quickly made it clear that this wasn’t going to help us. (Granted, I could’ve gotten there by carefully reading the paper, too, but I was so excited to have something to play with that I didn’t give it more than a cursory glance.) FoodBase was a corpus in English with annotated food words – and to train a named-entity recognition (NER) model to recognize food in the Baby-Sitters Club, you’d need a corpus with annotated food words. (Curious what the deal is with NER models and the SpaCy natural language processing package we tried out on English and French? Check out <a class="reference external" href="https://datasittersclub.github.io/site/dscm2.html">DSC Multilingual Mystery #2: Beware, Lee and Quinn</a>!) But NER models work best on texts that are <em>similar</em> to what they’re trained on. Most NER models are trained on corpora that involve lots of news and Wikipedia pages, which can be very different from literature. That’s why David Bamman and collaborators developed <a class="reference external" href="https://github.com/dbamman/litbank">LitBank</a>, a data set with excerpts from English-language literature, annotated for entities and other things. When LitBank was used as the training data for an NER model, it performed better than generic models when applied to literature. Pretty cool, huh? The problem was that FoodBase was annotating the kinds of entities we cared about (food) in a kind of document even <strong>less</strong> like our texts than news articles or Wikipedia: it was using recipes.</p>
<p>There were other problems, too. Given the importance of annotated data for supervised machine learning tasks (i.e. setups where you give the computer a bunch of labeled things and make it try to apply those labels to new data), I assumed that there’d be some agreed-upon, cross-disciplinary standard for how to format that data. And boy, was I ever wrong!  FoodBank used <a class="reference external" href="http://bioc.sourceforge.net/">BioC</a>, an XML-based format for storing annotations. You can kind of imagine BioC as being like TEI for biology and medicine. (What are XML and TEI? Check out <a class="reference external" href="https://datasittersclub.github.io/site/dsc5.html">DSC #5: The DSC and the Impossible TEI Quandaries</a>.) But much like how TEI plays a major role in a certain kind of humanities annotation, but just gets you blank stares if you bring it up in most computer science circles, BioC isn’t a data format that you can just give to SpaCy to make it train a model. And I’m a little embarrassed to admit how deep I dug myself into the hole of the BioC-o-sphere, reading all the documentation I could find and trying to daisy-chain <a class="reference external" href="https://github.com/Aitslab/BioNLP">tools for converting it to other formats</a>, before I realized that there was no point. Middle-grade novels like the Baby-Sitters Club are very, very different texts than recipes. We needed a different approach.</p>
</div>
<div class="section" id="edible-genealogies">
<h3>Edible Genealogies<a class="headerlink" href="#edible-genealogies" title="Permalink to this headline">¶</a></h3>
<p>As far as I could tell, no one had done what we were setting out to do: using computation to identify food references in middle-grade novels. (Or in any literature, as far as we could tell. And yeah, we stand behind the idea that the Baby-Sitters Club counts as literature.) We needed to compile our own data, and come up with our own workflow.</p>
<p>As I puzzled it over, one resource for English that came to mind was <a class="reference external" href="https://wordnet.princeton.edu/">WordNet</a>, which describes itself as “a lexical database for English”. Basically, it’s an attempt to organize much of the English language using semantic concepts. Words are tagged with these abstract concepts, like “physical entity” or “attribute”, and are organized into hierarchical relationships, so that more specific terms (like <em>child</em>) have more general “parent” terms (like <em>juvenile person</em>). And this happens recursively, leading to concepts that ultimately get pretty abstract: child → juvenile person→ person → organism → living thing → object → physical entity → entity.</p>
<p>What if I pulled out all the nouns in the original English versions of the Baby-Sitters Club, ran them through WordNet, and then grabbed the ones that were labeled as “food” anywhere in their hierarchy?</p>
<p>I ran a few examples through the WordNet web interface to see if this could work.</p>
<p>It seemed promising:</p>
<p>* Cheese → dairy product → foodstuff → <strong>food</strong></p>
<ul class="simple">
<li><p>Lemonade → fruit drink → beverage → <strong>food</strong></p></li>
<li><p>Pizza → dish → nutriment → <strong>food</strong></p></li>
</ul>
<p>Until I ran into fruits, berries, and other plant matter:</p>
<ul class="simple">
<li><p>Strawberry → fruit → reproductive structure → plant organ → plant part → natural object → whole → object → physical entity → entity</p></li>
<li><p>Cherry → wood → plant material → material → substance → matter → physical entity → entity</p></li>
</ul>
<p>There were other words that also ran off in weird directions.</p>
<p>The first definition of “hotdog” is “someone who performs dangerous stunts to attract attention to himself”, which gives us the hierarchy <em>hotdog → exhibitionist → egotist → unpleasant person → unwelcome person → person → organism → living thing → whole → object → physical entity → entity</em>.</p>
<p>The first definition of “cake” is “a block of solid substance (such as soap or wax)”: <em>cake → block → artifact → whole → object → physical entity → entity</em>. (NLP people, are you okay? Do you need someone to bake you a cake?)</p>
<p>We only had all four French translations for 19 books, so I went through the words that WordNet had initially declared non-food and manually added the ones that were missing. But this was going to be a bigger problem as we scaled up, so eventually, with some coding syntax help from my colleague Simon Wiles, I ended up looping through all the possible definitions to look for food tags, and that worked out much better.</p>
<div class="section" id="install-modules">
<h4>Install modules<a class="headerlink" href="#install-modules" title="Permalink to this headline">¶</a></h4>
<p>The first step is to import a few modules: <code class="docutils literal notranslate"><span class="pre">os</span></code> for navigating around the filesystem, and <code class="docutils literal notranslate"><span class="pre">nltk</span></code> for extracting just the nouns, and checking them against WordNet.</p>
<p>NLTK is short for Natural Language Toolkit, and is a common (though older) natural language processing (NLP) package for Python. It’s used a lot in computer science classrooms for students to learn how NLP works, and while it does include some corpora in other languages, it doesn’t have usable-out-of-the-box tools for those languages like it does for English. Luckily, we’re starting off here by working in English.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#OS is used to navigate the filesystem</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="c1">#NLTK is used to extract nouns and check against WordNet</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="c1">#Download the WordNet data</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>
<span class="c1">#Import the WordNet data</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">wordnet</span> <span class="k">as</span> <span class="n">wn</span>
<span class="c1">#Import the lemmatizer (to turn words into a form we will find in WordNet)</span>
<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package wordnet to /Users/qad/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="choose-the-directory">
<h4>Choose the directory<a class="headerlink" href="#choose-the-directory" title="Permalink to this headline">¶</a></h4>
<p>Specify the full path to the directory with the files you want to work with, then navigate to that directory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Specify the full path to the directory with the files you want to work with.</span>
<span class="c1">#On Mac it looks like what you see below</span>
<span class="c1">#On Windows it looks more like &#39;C:\\Users\\YOUR-USER-NAME\\Documents&#39;</span>
<span class="n">textdir</span> <span class="o">=</span> <span class="s1">&#39;/Users/qad/Documents/dsc_corpus_clean&#39;</span>
<span class="c1">#Move to that directory</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">textdir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="find-the-nouns">
<h4>Find the nouns<a class="headerlink" href="#find-the-nouns" title="Permalink to this headline">¶</a></h4>
<p>The following code creates a list (called <em>bscnouns</em>), opens each text file in the folder you specified, reads the text of each file, separates out the sentences, and for each sentence, checks each word in the sentence to see if it’s a noun. If it’s a noun, that word gets written to the <em>bscnouns</em> list.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Creates the bscnouns list</span>
<span class="n">bscnouns</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1">#For each file in the directory you listed above...</span>
<span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">textdir</span><span class="p">):</span>
    <span class="c1">#If the file is a text file</span>
    <span class="k">if</span> <span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.txt&#39;</span><span class="p">):</span>
        <span class="c1">#Open the file</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="c1">#Read the text of the file</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
            <span class="c1">#Split the text into sentences</span>
            <span class="n">sentences</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="c1">#For each sentence...</span>
            <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
                <span class="c1">#For each word in the sentence...</span>
                <span class="k">for</span> <span class="n">word</span><span class="p">,</span><span class="n">pos</span> <span class="ow">in</span> <span class="n">nltk</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">sentence</span><span class="p">))):</span>
                    <span class="c1">#If the word is tagged with a noun part-of-speech...</span>
                    <span class="k">if</span> <span class="p">(</span><span class="n">pos</span> <span class="o">==</span> <span class="s1">&#39;NN&#39;</span> <span class="ow">or</span> <span class="n">pos</span> <span class="o">==</span> <span class="s1">&#39;NNS&#39;</span><span class="p">):</span>
                        <span class="c1">#Add the word to the nouns list</span>
                        <span class="n">bscnouns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="data-cleaning">
<h4>Data cleaning<a class="headerlink" href="#data-cleaning" title="Permalink to this headline">¶</a></h4>
<p>From here, we want to lemmatize and lower-case all the nouns so that they’ll match the words in the WordNet database. (Lemmatizing involves changing a word to its dictionary form, and it’s especially useful for languages that have a lot of inflection.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Use the WordNet lemmatizer that we imported earlier</span>
<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="c1">#Creates a new list of lemmas</span>
<span class="n">bsclemmas</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1">#For every noun in our list of nouns</span>
<span class="k">for</span> <span class="n">bscnoun</span> <span class="ow">in</span> <span class="n">bscnouns</span><span class="p">:</span>
    <span class="c1">#Lemmatize it</span>
    <span class="n">bsclemma</span> <span class="o">=</span> <span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">bscnoun</span><span class="p">)</span>
    <span class="c1">#Lowercase it</span>
    <span class="n">bsclemma</span> <span class="o">=</span> <span class="n">bsclemma</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="c1">#Add that lemma to the lemmas list</span>
    <span class="n">bsclemmas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bsclemma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="finding-unique-words">
<h4>Finding unique words<a class="headerlink" href="#finding-unique-words" title="Permalink to this headline">¶</a></h4>
<p>We don’t want to look up the words in WordNet over and over and over again, every time they appear. What we want is a set of <em>unique</em> words, so we only look each word up once.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Create a set of unique words</span>
<span class="n">unique_lemma_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">bsclemmas</span><span class="p">)</span>
<span class="c1">#Turn it into a list</span>
<span class="n">unique_lemmas</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">unique_lemma_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>While we’re at it, let’s do counts of the nouns just for fun.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#This package is useful for counting things</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="c1">#Sort the nouns by frequency</span>
<span class="n">ctr</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">bsclemmas</span><span class="p">)</span>
<span class="c1">#Print them</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ctr</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="running-words-through-wordnet">
<h4>Running words through WordNet<a class="headerlink" href="#running-words-through-wordnet" title="Permalink to this headline">¶</a></h4>
<p>With WordNet, we want to catch the possibility of the ‘food’ label occurring at any level. Maybe the word itself is labeled food. Or maybe its parent term is labeled food. Or maybe <strong>that word’s</strong> parent term is labeled food. And so on, all the way up the hierarchy.</p>
<p>The parent terms are called “hypernyms”, and what this code does is check and see if the word has a hypernym. If it doesn’t have 0 hypernyms (the <code class="docutils literal notranslate"><span class="pre">!=</span></code> means ‘does not equal’), it grabs the first hypernym, then checks if <em>that</em> word has a hypernym. And so it continues up to the great-great-great-great-great-great-great-great-grandparent term; I never found anything nested more deeply than that.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Creates a text file output called parent-terms</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;parent-terms.tsv&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">out</span><span class="p">:</span>
    <span class="c1">#Writes a header: &#39;word&#39;, a tab, then &#39;parents&#39;</span>
    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;word&#39;</span><span class="p">)</span>
    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;parents&#39;</span><span class="p">)</span>
    <span class="c1">#For each unique word in our list</span>
    <span class="k">for</span> <span class="n">unique_lemma</span> <span class="ow">in</span> <span class="n">unique_lemmas</span><span class="p">:</span>
        <span class="c1">#Counts the number of definitions for the word</span>
        <span class="n">synsets</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">wn</span><span class="o">.</span><span class="n">synsets</span><span class="p">(</span><span class="n">unique_lemma</span><span class="p">))</span>
        <span class="c1">#For each of the definitions</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">synsets</span><span class="p">):</span>
            <span class="c1">#Write out the word from our list, then a tab</span>
            <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">unique_lemma</span><span class="p">)</span>
            <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="c1">#Defines the word as the current definition we&#39;re looking at</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">wn</span><span class="o">.</span><span class="n">synsets</span><span class="p">(</span><span class="n">unique_lemma</span><span class="p">)[</span><span class="n">x</span><span class="p">]</span>
            <span class="c1">#Writes out the immediate parent terms (hypernyms)</span>
            <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()))</span>
            <span class="c1">#If there&#39;s more than 0 immediate parent terms</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">())</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1">#Defines parent word as the first hypernym</span>
                <span class="c1">#Python starts counting with 0, sorry, so here 0 means first</span>
                <span class="n">parentword</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                <span class="c1">#Writes out a tab</span>
                <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
                <span class="c1">#Writes out the immediate parent terms of the parent term</span>
                <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">parentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()))</span>
                <span class="c1">#Repeats all that for the next level</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">())</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grandparentword</span> <span class="o">=</span> <span class="n">parentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
                    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()))</span>
                    <span class="c1">#Repeats all that for the next level</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">())</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">greatgrandparentword</span> <span class="o">=</span> <span class="n">grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
                        <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">greatgrandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()))</span>
                        <span class="c1">#Repeats all that for the next level</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">greatgrandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">())</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">g2grandparentword</span> <span class="o">=</span> <span class="n">greatgrandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                            <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
                            <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">g2grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()))</span>
                            <span class="c1">#Repeats all that for the next level</span>
                            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">g2grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">())</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                                <span class="n">g3grandparentword</span> <span class="o">=</span> <span class="n">g2grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                                <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
                                <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">g3grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()))</span>
                                <span class="c1">#Repeats all that for the next level</span>
                                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">g3grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">())</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                                    <span class="n">g4grandparentword</span> <span class="o">=</span> <span class="n">g3grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                                    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
                                    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">g4grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()))</span>
                                    <span class="c1">#Repeats all that for the next level</span>
                                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">g4grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">())</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                                        <span class="n">g5grandparentword</span> <span class="o">=</span> <span class="n">g4grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                                        <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
                                        <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">g5grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()))</span>
                                        <span class="c1">#Repeats all that for the next level (are you bored yet?)</span>
                                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">g5grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">())</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                                            <span class="n">g6grandparentword</span> <span class="o">=</span> <span class="n">g5grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                                            <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
                                            <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">g6grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()))</span>
                                            <span class="c1">#Repeats all that for the next level</span>
                                            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">g6grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">())</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                                                <span class="n">g7grandparentword</span> <span class="o">=</span> <span class="n">g6grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                                                <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
                                                <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">g7grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()))</span>
                                                <span class="c1">#Seriously last time repeats all that for the next level</span>
                                                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">g7grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">())</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                                                    <span class="n">g8grandparentword</span> <span class="o">=</span> <span class="n">g7grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
                                                    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">)</span>
                                                    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">g8grandparentword</span><span class="o">.</span><span class="n">hypernyms</span><span class="p">()))</span>
            <span class="c1">#Newline before the next word from our list!</span>
            <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="extracting-the-food-words">
<h4>Extracting the food words<a class="headerlink" href="#extracting-the-food-words" title="Permalink to this headline">¶</a></h4>
<p>Now that we had a big text file with all the (great-great-great-whatever-)parent terms of all our nouns, all we need to do is extract the ones that are labeled ‘food’ somewhere, then pretty up the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Regular expressions pattern is useful for matching text</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="c1">#Opens the text file we created in the last step</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;parent-terms.tsv&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="c1">#Creates a new text file for just the food terms</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;food-terms.tsv&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">foodout</span><span class="p">:</span>
        <span class="c1">#Reads each line of the text file we just created</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
        <span class="c1">#For each line...</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
            <span class="c1">#Does the word food occur? If it occurs at least once:</span>
            <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\&#39;food\.&quot;</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1">#Write that line out to our output file</span>
                <span class="n">foodout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#CSV can be handy for reading and writing CSVs and other delineated files</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="c1">#Open the food terms file we just created</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;food-terms.tsv&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="c1">#Create a new text file for the clean output</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;food-terms-clean.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">cleanfoodout</span><span class="p">:</span>
        <span class="c1">#Read in our file of food terms, specifying it uses tabs to separate out the word from the parent terms</span>
        <span class="n">reader</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1">#For each row</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
            <span class="c1">#Write a space to the output file</span>
            <span class="n">cleanfoodout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
            <span class="c1">#Write just the original word out to our output file</span>
            <span class="n">cleanfoodout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="c1">#Write another space to the output file</span>
            <span class="c1">#These spaces make sure that when we search for these words later</span>
            <span class="c1">#We don&#39;t get them as part of another word</span>
            <span class="c1">#So we don&#39;t get non-food words like &#39;appear&#39; because it has &#39;pear&#39; in it</span>
            <span class="n">cleanfoodout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
            <span class="c1">#Add a newline character</span>
            <span class="n">cleanfoodout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With my food-word list ready, I had what I needed for us to move on to the next step of this project: aligning the original English texts with their French translations, and extracting English/French sentence pairs where the English contained one of my identified food words. But I had some new toys that I wanted to play with, so off I ran towards the siren song of training an NER model.</p>
</div>
</div>
<div class="section" id="why-train-an-ner-model">
<h3>Why Train an NER Model?<a class="headerlink" href="#why-train-an-ner-model" title="Permalink to this headline">¶</a></h3>
<p>The approach I’d taken so far – where I used WordNet plus manual review to create a list of food words to look for – only works up to a certain scale. The Baby-Sitters Club English corpus only contains around 6 million words, and only about 1 million of them were nouns as identified by NLTK. It’s an annoying but doable human scale, especially with a little computational help from WordNet.</p>
<p>The problem shows up when you want to look at thousands of texts or more. As we mentioned in <a class="reference external" href="https://datasittersclub.github.io/site/dsc10.html#pca-across-series">DSC #10: Heather Likes Principal Component Analysis</a>, I’ve also been building a larger corpus of middle-grade and YA books, including lots of series books. The food list we got out of the BSC books would probably cover most of the food references for realist mainstream American kid fiction from the 80’s and 90’s: popsicles, casseroles, sandwiches, pizza. But times change, food trends change, kid fiction is getting more racially and ethnically diverse, and food words like kimchi or paneer just aren’t on our list. I could repeat the time-consuming process of running WordNet then manually checking the untagged words for a much larger set of works, but that’s not the only way out of this conundrum. Instead of trying to look for foods using a defined list of words, I could annotate some examples of food sentences, and train an NER model to look for <em>context</em>.</p>
<p>That’s how NER works: it’s not trying to find places based on a gazetteer, or names based on a list of common personal names, it uses what it can figure out from its training data about how names and places tend to show up in sentences to predict that something new is a place or name. NER doesn’t happen in isolation: it’s one of the later steps in a natural-language processing (NLP) pipeline like spaCy, which is what I usually use with English, French, Spanish, and the <a class="reference external" href="https://spacy.io/models">other languages it supports</a>. Before spaCy tries to guess what’s a person or place, it’s already done its homework. It’s <em>tokenized</em> the text, differentiating the words from the punctuation. Then it’s <em>tagged</em> the text with part-of-speech annotations. This is important because it means spaCy can automatically rule out things like verbs, adverbs, prepositions, and punctuation from being possible names or locations. After that, spaCy <em>parses</em> the text, identifying the syntactic relationships between the words. This is useful because we know, for instance, that people’s names are often the subject of sentences, and that locations often follow prepositions like “in” or “to”. And only after building up all that contextual information does spaCy try to do NER.</p>
<p>The result can be really powerful, and that’s what I was hoping for! I figured that if I gave it enough examples of things like “I ate [food]” or “Claudia opened a [container] of [food]” or “Mom finished cooking the [food]”, it’d pick up on those patterns in a way that would work for very different foods that it encountered in other books and series.</p>
<p>And, to be totally honest, I also thought it’d make the technical section of our conference submissions sound cooler to say we were training a food NER model vs. “we just searched for food words we identified in our corpus using WordNet”. (We put in a proposal for the Canadian DH association, CSDH-SCHN, in January 2021.) I can’t actually endorse choosing your method because you think it’ll make your conference submissions sound better. I was really convinced that training an NER model would work, and would also help bootstrap expanding this project to a much larger corpus. But I had no idea what I was in for when I actually tried it.</p>
</div>
<div class="section" id="why-not-train-an-ner-model">
<h3>Why not train an NER model?<a class="headerlink" href="#why-not-train-an-ner-model" title="Permalink to this headline">¶</a></h3>
<div class="section" id="installing-prodigy">
<h4>Installing Prodigy<a class="headerlink" href="#installing-prodigy" title="Permalink to this headline">¶</a></h4>
<p>To take my ill-fated trip down the path of training an NER model, I used Prodigy, an annotation tool developed by the creator of spaCy. It’s not cheap ($400 for a personal copy and a year of updates), but it’s a lot easier to set up and run than the many of the annotation tools out there. Literally, you just download the software file (which is a particular kind of Python installer called a wheel, or .whl), open a terminal, navigate to where you saved the installer and run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">./prodigy*.whl</span></code></p>
</div>
<div class="section" id="preparing-a-sample-for-annotation">
<h4>Preparing a Sample for Annotation<a class="headerlink" href="#preparing-a-sample-for-annotation" title="Permalink to this headline">¶</a></h4>
<p>I prepared a text file with a thousand sentences from the BSC that had food in them, using the list of food words we just generated using WordNet:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Tokenizer to split texts into sentences</span>
<span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">tokenize</span>
<span class="c1">#Does random selection</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="c1">#Specify the full path to the directory with the files you want to work with.</span>
<span class="c1">#On Mac it looks like what you see below</span>
<span class="c1">#On Windows it looks more like &#39;C:\\Users\\YOUR-USER-NAME\\Documents&#39;</span>
<span class="n">textdir</span> <span class="o">=</span> <span class="s1">&#39;/Users/qad/Documents/dsc_corpus_clean&#39;</span>
<span class="c1">#Move to that directory</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">textdir</span><span class="p">)</span>
<span class="c1">#Open the file with all the food terms</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;/Users/qad/Documents/dsc-food-terms.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">foodfile</span><span class="p">:</span>
    <span class="c1">#Read it into a list called foods</span>
    <span class="n">foods</span> <span class="o">=</span> <span class="n">foodfile</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
    <span class="c1">#Print the list to make sure we&#39;re getting what we want</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">foods</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39; alcohol &#39;, &#39; ale &#39;, &#39; alfalfa &#39;, &#39; ambrosia &#39;, &#39; applesauce &#39;, &#39; bacon &#39;, &#39; bagel &#39;, &#39; bagels &#39;, &#39; baklava &#39;, &#39; bar &#39;, &#39; barbecue &#39;, &#39; barley &#39;, &#39; bean &#39;, &#39; beef &#39;, &#39; beer &#39;, &#39; beet &#39;, &#39; beverage &#39;, &#39; biscuit &#39;, &#39; bologna &#39;, &#39; bonbon &#39;, &#39; bourguignon &#39;, &#39; brain &#39;, &#39; brains &#39;, &#39; bran &#39;, &#39; bread &#39;, &#39; broccoli &#39;, &#39; brownie &#39;, &#39; brownies &#39;, &#39; bun &#39;, &#39; burger &#39;, &#39; burgers &#39;, &#39; burrito &#39;, &#39; butter &#39;, &#39; buttermilk &#39;, &#39; butterscotch &#39;, &#39; cabbage &#39;, &#39; cake &#39;, &#39; candy &#39;, &#39; cappuccino &#39;, &#39; caramel &#39;, &#39; carob &#39;, &#39; carrot &#39;, &#39; casserole &#39;, &#39; catsup &#39;, &#39; caviar &#39;, &#39; celery &#39;, &#39; cereal &#39;, &#39; champagne &#39;, &#39; cheddar &#39;, &#39; cheese &#39;, &#39; cheeseburger &#39;, &#39; cheesecake &#39;, &#39; chicken &#39;, &#39; chili &#39;, &#39; chip &#39;, &#39; chips &#39;, &#39; chocolate &#39;, &#39; chow &#39;, &#39; chowder &#39;, &#39; cider &#39;, &#39; cinnamon &#39;, &#39; cocktail &#39;, &#39; cocoa &#39;, &#39; coconut &#39;, &#39; coffee &#39;, &#39; coke &#39;, &#39; cola &#39;, &#39; comfrey &#39;, &#39; condiment &#39;, &#39; cookie &#39;, &#39; cookies &#39;, &#39; coriander &#39;, &#39; corn &#39;, &#39; cornbread &#39;, &#39; cornmeal &#39;, &#39; courgette &#39;, &#39; courgettes &#39;, &#39; crab &#39;, &#39; cracker &#39;, &#39; crackers &#39;, &#39; cream &#39;, &#39; crepe &#39;, &#39; croquette &#39;, &#39; cucumber &#39;, &#39; cupcake &#39;, &#39; cupcakes &#39;, &#39; curd &#39;, &#39; danish &#39;, &#39; dessert &#39;, &#39; dill &#39;, &#39; dings &#39;, &#39; dip &#39;, &#39; donut &#39;, &#39; doritos &#39;, &#39; doughnut &#39;, &#39; dumpling &#39;, &#39; eclair &#39;, &#39; egg &#39;, &#39; eggplant &#39;, &#39; eggplants &#39;, &#39; eggs &#39;, &#39; enchilada &#39;, &#39; feast &#39;, &#39; fillet &#39;, &#39; filling &#39;, &#39; fish &#39;, &#39; flour &#39;, &#39; fondue &#39;, &#39; fowl &#39;, &#39; fries &#39;, &#39; fritos &#39;, &#39; fritter &#39;, &#39; fudge &#39;, &#39; garlic &#39;, &#39; ginger &#39;, &#39; gingerbread &#39;, &#39; goulash &#39;, &#39; graham &#39;, &#39; granola &#39;, &#39; gravy &#39;, &#39; guacamole &#39;, &#39; gum &#39;, &#39; gumdrop &#39;, &#39; gummi &#39;, &#39; ham &#39;, &#39; hamburger &#39;, &#39; hamburgers &#39;, &#39; herb &#39;, &quot; hershey&#39;s &quot;, &#39; hoagie &#39;, &#39; hoagy &#39;, &#39; honey &#39;, &#39; hotdog &#39;, &#39; hummus &#39;, &#39; ice &#39;, &#39; iceberg &#39;, &#39; jam &#39;, &#39; jawbreaker &#39;, &#39; jell-o &#39;, &#39; jelly &#39;, &#39; jerky &#39;, &#39; juice &#39;, &#39; kale &#39;, &#39; ketchup &#39;, &#39; lamb &#39;, &#39; lasagna &#39;, &#39; lemonade &#39;, &#39; lettuce &#39;, &#39; licorice &#39;, &#39; limeade &#39;, &#39; liver &#39;, &#39; liverwurst &#39;, &#39; loaf &#39;, &#39; lobster &#39;, &#39; lollies &#39;, &#39; lollipop &#39;, &#39; lox &#39;, &#39; macaroni &#39;, &#39; malt &#39;, &#39; margarine &#39;, &#39; marshmallow &#39;, &#39; marshmallows &#39;, &#39; marzipan &#39;, &#39; mayonnaise &#39;, &#39; meat &#39;, &#39; meatball &#39;, &#39; meatloaf &#39;, &#39; milk &#39;, &#39; milky &#39;, &#39; millet &#39;, &#39; mint &#39;, &#39; mints &#39;, &#39; mixture &#39;, &#39; molasses &#39;, &#39; mousse &#39;, &#39; muffin &#39;, &#39; mushroom &#39;, &#39; mustard &#39;, &#39; m&amp;ms &#39;, &#39; ms &#39;, &#39; musketeers &#39;, &#39; nan &#39;, &#39; neccos &#39;, &#39; noodle &#39;, &#39; oatmeal &#39;, &#39; okra &#39;, &#39; omelet &#39;, &#39; omelette &#39;, &#39; onion &#39;, &#39; onions &#39;, &#39; oregano &#39;, &#39; oreo &#39;, &#39; oreos &#39;, &#39; oyster &#39;, &#39; pancake &#39;, &#39; pancakes &#39;, &#39; parfait &#39;, &#39; parsley &#39;, &#39; parsnip &#39;, &#39; pasta &#39;, &#39; pastry &#39;, &#39; pea &#39;, &#39; pepper &#39;, &#39; peppermint &#39;, &#39; pepperoni &#39;, &#39; pie &#39;, &#39; pimento &#39;, &#39; pita &#39;, &#39; pizza &#39;, &#39; pop &#39;, &#39; popcorn &#39;, &#39; popsicle &#39;, &#39; popsicles &#39;, &#39; pork &#39;, &#39; porridge &#39;, &#39; potato &#39;, &#39; praline &#39;, &#39; pretzel &#39;, &#39; pretzels &#39;, &#39; pudding &#39;, &#39; pumpernickel &#39;, &#39; pumpkin &#39;, &#39; quiche &#39;, &#39; radicchio &#39;, &#39; radish &#39;, &#39; ravioli &#39;, &#39; relish &#39;, &#39; rib &#39;, &#39; rice &#39;, &#39; roast &#39;, &#39; roll &#39;, &#39; salad &#39;, &#39; salami &#39;, &#39; salmon &#39;, &#39; salsa &#39;, &#39; salt &#39;, &#39; saltine &#39;, &#39; sandwich &#39;, &#39; sandwiches &#39;, &#39; sardine &#39;, &#39; sardines &#39;, &#39; sauce &#39;, &#39; sauerkraut &#39;, &#39; sausage &#39;, &#39; seafood &#39;, &#39; seasoning &#39;, &#39; seltzer &#39;, &#39; sherbet &#39;, &#39; shortcake &#39;, &#39; shrimp &#39;, &#39; slaw &#39;, &#39; smoothie &#39;, &#39; smorgasbord &#39;, &#39; snowball &#39;, &#39; snickers &#39;, &#39; soda &#39;, &#39; soup &#39;, &#39; soy &#39;, &#39; soybean &#39;, &#39; soybeans &#39;, &#39; spaghetti &#39;, &#39; spice &#39;, &#39; spinach &#39;, &#39; sprout &#39;, &#39; sprouts &#39;, &#39; spud &#39;, &#39; squash &#39;, &#39; steak &#39;, &#39; stew &#39;, &#39; stuffing &#39;, &#39; succotash &#39;, &#39; sugar &#39;, &#39; sundae &#39;, &#39; supper &#39;, &#39; sushi &#39;, &#39; sweet &#39;, &#39; sweetener &#39;, &#39; swizzle &#39;, &#39; syrup &#39;, &#39; taco &#39;, &#39; tacos &#39;, &#39; taffy &#39;, &#39; tahini &#39;, &#39; tapioca &#39;, &#39; tea &#39;, &#39; toast &#39;, &#39; tofu &#39;, &#39; tomato &#39;, &#39; tootsie &#39;, &#39; tortilla &#39;, &#39; tostada &#39;, &#39; trout &#39;, &#39; tuna &#39;, &#39; turkey &#39;, &#39; turnip &#39;, &#39; twinkie &#39;, &#39; twinkies &#39;, &#39; vanilla &#39;, &#39; veal &#39;, &#39; veg &#39;, &#39; vegetable &#39;, &#39; veggie &#39;, &#39; vinegar &#39;, &#39; waffle &#39;, &#39; waffles &#39;, &#39; watercress &#39;, &#39; wheat &#39;, &#39; wine &#39;, &quot; wrigley&#39;s &quot;, &#39; yankee &#39;, &#39; yodels &#39;, &#39; yogurt &#39;, &#39; yolk &#39;, &#39; ziti &#39;, &#39; zucchini &#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Creates a list for the food sentences</span>
<span class="n">foodsentences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1">#For each file in the directory you specified</span>
<span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">textdir</span><span class="p">):</span>
    <span class="c1">#If the filename ends with .txt</span>
    <span class="k">if</span> <span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.txt&#39;</span><span class="p">):</span>
        <span class="c1">#Open the text file</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="c1">#Read the text</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
            <span class="c1">#Do the sentence tokenization, put sentences in a list</span>
            <span class="n">sentences</span> <span class="o">=</span> <span class="n">tokenize</span><span class="o">.</span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
            <span class="c1">#For each sentence in the list</span>
            <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
                <span class="c1">#If there&#39;s a match for any of the food words</span>
                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">food</span> <span class="ow">in</span> <span class="n">sentence</span> <span class="k">for</span> <span class="n">food</span> <span class="ow">in</span> <span class="n">foods</span><span class="p">):</span>
                    <span class="c1">#Add the sentence to the foodsentences list</span>
                    <span class="n">foodsentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="c1">#Creates a new list with a random sample of 999 sentences with food</span>
<span class="c1">#Don&#39;t forget Python starts counting with 0</span>
<span class="n">randomfoods</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">foodsentences</span><span class="p">,</span> <span class="mi">999</span><span class="p">)</span>
<span class="c1">#Opens an output file for the random sentences</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;randomfood.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">randomout</span><span class="p">:</span>
    <span class="c1">#For each random food sentence in the list</span>
    <span class="k">for</span> <span class="n">randomfood</span> <span class="ow">in</span> <span class="n">randomfoods</span><span class="p">:</span>
        <span class="c1">#Write out the sentence</span>
        <span class="n">randomout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">randomfood</span><span class="p">)</span>
        <span class="c1">#Write out a new line</span>
        <span class="n">randomout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="annotating-with-prodigy">
<h4>Annotating with Prodigy<a class="headerlink" href="#annotating-with-prodigy" title="Permalink to this headline">¶</a></h4>
<p>I navigated to the folder that has my randomfood.txt file, and ran this command:</p>
<p><code class="docutils literal notranslate"><span class="pre">prodigy</span> <span class="pre">ner.manual</span> <span class="pre">ner_yams_food</span> <span class="pre">en_coreweb_sm</span> <span class="pre">./randomfood.txt</span> <span class="pre">--label</span> <span class="pre">FOOD</span></code></p>
<p>Breaking it down:</p>
<ul class="simple">
<li><p><em>prodigy</em> says what software I’m trying to launch.</p></li>
<li><p>Prodigy lets you do annotation to train different parts of the spaCy pipeline, and the <a class="reference external" href="https://prodi.gy/docs/recipes#ner-manual">part I want to train is NER</a>.</p></li>
<li><p>I’m calling the annotation project <code class="docutils literal notranslate"><span class="pre">ner_yams_food</span></code> (YAMS because I wanted to use more than just the BSC, but also other texts from my Young Adult and Middle-School corpus).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">en_coreweb_sm</span></code> is the English spaCy model I’m using for the other parts of the NER pipeline.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">./randomfood.txt</span></code> is the list of sentences I want to annotate.</p></li>
<li><p>And <code class="docutils literal notranslate"><span class="pre">--label</span> <span class="pre">FOOD</span></code> indicates that the label I want to be able to annotate the texts with is “Food”.</p></li>
</ul>
<p>And then (complete with magic sparkles), the terminal said:
<code class="docutils literal notranslate"><span class="pre">✨</span>&#160; <span class="pre">Starting</span> <span class="pre">the</span> <span class="pre">web</span> <span class="pre">server</span> <span class="pre">at</span> <span class="pre">http://localhost:8080</span> <span class="pre">...</span> <span class="pre">Open</span> <span class="pre">the</span> <span class="pre">app</span> <span class="pre">in</span> <span class="pre">your</span> <span class="pre">browser</span> <span class="pre">and</span> <span class="pre">start</span> <span class="pre">annotating!</span></code></p>
<p>I opened the browser, put <code class="docutils literal notranslate"><span class="pre">http://localhost:8080</span></code> into the address bar, and saw this interface.</p>
<p><img alt="Prodigy interface" src="_images/dscm4_prodigy.png" /></p>
<p>All I needed to do was highlight the words that were food, then hit the green check button at the bottom, which would take me to the next sentence.</p>
<p><img alt="Prodigy interface" src="_images/dscm4_prodigy_annotated.png" /></p>
<p>If any sentences had gotten in by mistake, I could hit the red X button to mark that there were no foods, and I could hit the gray crossed-out circle button if I wanted to skip a sentence</p>
</div>
<div class="section" id="consistency-consistency-consistency">
<h4>Consistency, Consistency, Consistency<a class="headerlink" href="#consistency-consistency-consistency" title="Permalink to this headline">¶</a></h4>
<p>I annotated and annotated and annotated. The more I annotated, the more decisions I had to make – and some of them weren’t trivial. Ultimately, I decided words for meals should be excluded, because they were events more than food. And particularly for the Baby-Sitters Club, which often discusses Dawn’s diabetes, “sugar” should also be excluded because it’s almost always about “blood sugar”. I didn’t just want BSC books; I also brought in some sentences from <em>Sweet Valley High</em>, where I discovered that those texts are much less food-obsessed, but seem to place a greater emphasis on ice cream and milkshakes than the BSC. And I kept annotating.</p>
</div>
<div class="section" id="model-training">
<h4>Model Training<a class="headerlink" href="#model-training" title="Permalink to this headline">¶</a></h4>
<p>Finally, I had over a thousand sentences, and I thought I’d try to train the model. But first, I wanted to test out Prodigy’s <em>learning curve</em> feature, which tests what it would look like to train the model with different amounts of data. If the model quality keeps improving (as measured by the model’s ability to accurately predict food values for sentences you annotated, but Prodigy hid from itself while training on a different set of sentences), you need more data.</p>
<p>I had to run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">spacy-lookups-data</span></code> beforehand to keep Prodigy happy, but then I ran the training curve command:</p>
<p><code class="docutils literal notranslate"><span class="pre">prodigy</span> <span class="pre">train-curve</span> <span class="pre">-l</span> <span class="pre">en</span> <span class="pre">-n</span> <span class="pre">ner_yams_food</span> <span class="pre">-m</span> <span class="pre">en_core_web_sm</span></code></p>
<p>And got these results:</p>
</div>
</div>
<div class="section" id="text-alignment">
<h3>Text alignment<a class="headerlink" href="#text-alignment" title="Permalink to this headline">¶</a></h3>
<p>Getting lists of English sentences with food words in them is the first step – you have to find out what was there in the original. But the next step, text alignment, is also one that you should let the computer handle. Sure, you could open your English text in a text file, eyeball about how far through the book a given example is, and try to find the corresponding place in the translated text, but that sounds like a recipe for misery. Even if you split up both the original and translation so that each sentence appears on its own line, sentences get combined and split during the translation process all the time! It might help narrow the search, but it won’t quite get you what you need.</p>
<p>Luckily, text alignment algorithms exist! They’re not perfect, and sometimes they do weird and annoying things like dropping parts of one text or the other, but they’re solidly okay. There are multiple options, but the one that was easiest and most straightforward to run was <a class="reference external" href="https://github.com/rsennrich/Bleualign">Bleualign</a>, which uses machine translation as a way to sync up text versions.</p>
<p>You can’t just dump an entire novel and its machine translation onto Bleualign, though. Bleualign wants each sentence to be on its own line first, and then have <em>that</em> format be machine-translated. So first I ran the translations, and the English originals, through some code that split apart sentences and created a derivative file where each sentence was on its own line:</p>
<div class="section" id="importing-modules">
<h4>Importing modules<a class="headerlink" href="#importing-modules" title="Permalink to this headline">¶</a></h4>
<p>In case you didn’t import these already (or are running just this part of the code), let’s import <code class="docutils literal notranslate"><span class="pre">os</span></code> for navigating the filesystem and <code class="docutils literal notranslate"><span class="pre">tokenize</span></code> from NLTK.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#OS for navigating the filesystem</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="c1">#Tokenizer for splitting the texts into sentences</span>
<span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">tokenize</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="choosing-the-directory">
<h4>Choosing the directory<a class="headerlink" href="#choosing-the-directory" title="Permalink to this headline">¶</a></h4>
<p>This time, we need the directory with all our French translations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Put in the full path to your file directory</span>
<span class="n">frenchfiledirectory</span> <span class="o">=</span> <span class="s1">&#39;/Users/qad/Documents/dsc_french&#39;</span>
<span class="c1">#Move into that directory</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">filedirectory</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="splitting-text-into-sentences">
<h4>Splitting text into sentences<a class="headerlink" href="#splitting-text-into-sentences" title="Permalink to this headline">¶</a></h4>
<p>Like I mentioned before, NLTK’s out-of-the-box functionality is for English, so I wouldn’t use it to try to identify French nouns or anything. But here all we’re doing is splitting the French translations into sentences, and sentence tokenization works the same in French and English. (I wouldn’t try this on Japanese, where there’s different puncuation like 。and 、.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#For each file in the file directory you specified</span>
<span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">frenchfiledirectory</span><span class="p">):</span>
    <span class="c1">#If the file ends with .txt...</span>
    <span class="k">if</span> <span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.txt&#39;</span><span class="p">):</span>
        <span class="c1">#Create an output file name that adds -s at the end</span>
        <span class="c1">#-s is just for sentences, I made it up</span>
        <span class="n">outname</span> <span class="o">=</span> <span class="n">filename</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;-s.txt&#39;</span><span class="p">)</span>
        <span class="c1">#Open the text file from the directory</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="c1">#Open the output file you created</span>
            <span class="k">with</span> <span class="nb">open</span> <span class="p">(</span><span class="n">outname</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">out</span><span class="p">:</span>
                <span class="c1">#Read in the text file</span>
                <span class="n">text</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
                <span class="c1">#Do the sentence tokenization, put it in a list</span>
                <span class="n">sentences</span> <span class="o">=</span> <span class="n">tokenize</span><span class="o">.</span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
                <span class="c1">#For each sentence</span>
                <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
                    <span class="c1">#Write it to the output file</span>
                    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
                    <span class="c1">#Write a newline</span>
                    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="machine-translation-limits">
<h4>Machine translation limits<a class="headerlink" href="#machine-translation-limits" title="Permalink to this headline">¶</a></h4>
<p>Google keeps getting stingier, and as of 2021 it has a 2,800 character (meaning letters, spaces, and punctuation) limit if you try to paste text in the Google Translate text box. If you open a file using Google Docs, and go to Tools &gt; Translate document, the limit is larger (5,000 characters), but it’s not enough to translate a whole Baby-Sitters Club book. You can pay for Google cloud credits to be able to use the Google Translate API, but what you get then is <em>neither free nor easy</em>: you have to write code to break up your text into chunks no larger than 5,000 characters, send it to Google via the API, and stitch together the results you get back from Google. It’s a huge pain.</p>
<p>But I found a workaround using the <a class="reference external" href="https://chrome.google.com/webstore/detail/google-translate/aapbdbdomjkkjkaonfhkkikfgjllcleb/RK%3D2/RS%3DBBFW_pnWkPY0xPMYsAZI5xOgQEE-">Google Translate Chrome Add-On</a>! I took each of the Baby-Sitters Club translation text files that I had split into sentences, and opened them using Chrome. The Google Translate option popped up, and asked me if I wanted to translate the text into English. I said yes – but that wasn’t enough. It only would translate a little at a time, so I had to scroll slowly down to the bottom of the text to get a full translation of all of it. Then I copied and pasted the text into a new text file, and moved on to the next translation!</p>
<p>Doing it this way meant that I’d be using Bleualign a little bit backwards – the original English would be treated as the “translation”, the real translation into another language would be the “source”, with the machine-translation of the real translation back into English as a bridge between the two.</p>
</div>
<div class="section" id="bleualigning-all-the-things">
<h4>Bleualigning All The Things<a class="headerlink" href="#bleualigning-all-the-things" title="Permalink to this headline">¶</a></h4>
<p>To start with, go to the <a class="reference external" href="https://github.com/rsennrich/Bleualign">Bleualign GitHub page</a>, click the green Code button, and download the zip file. After you’ve unzipped it, open a command line, and type <code class="docutils literal notranslate"><span class="pre">cd</span></code> then the directory with the unzipped folder to move to that folder. (For me, it was <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">Downloads/bleualign</span></code>) Before you use Bleualign for the first time, you need to install it by running <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span></code> from inside the folder you unzipped with the Bleualign code.</p>
<p>The format of the command to run Bleualign looks like this:</p>
<p><code class="docutils literal notranslate"><span class="pre">./bleualign.py</span> <span class="pre">-s</span> <span class="pre">[french-translation.txt]</span> <span class="pre">-t</span> <span class="pre">[english-original.txt]</span> <span class="pre">--srctotarget</span> <span class="pre">[machine-translation-of-french-back-into-english.txt]</span> <span class="pre">-o</span> <span class="pre">[outputname]</span></code></p>
<p>For the sake of convenience, I put a copy of all my text files – the originals, the real translations, and the machine back-translations – in the Bleualign folder and ran all the alignment commands from there. Because I was going to be aligning multiple French translations to the same English text, I made myself a text file with the commands for each English book, where I could easily find-and-replace the filename variant for each of the translations, and copy and paste it into the command line. So the commands for aligning my Belgian translations were:</p>
<p><code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">./bleualign.py</span> <span class="pre">-s</span> <span class="pre">fr001_bg-s.txt</span> <span class="pre">-t</span> <span class="pre">001c_kristys_great_idea-s.txt</span> <span class="pre">--srctotarget</span> <span class="pre">fr001_bg-mt.txt</span> <span class="pre">-o</span> <span class="pre">fr001_bg-aligned</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">./bleualign.py</span> <span class="pre">-s</span> <span class="pre">fr002_bg-s.txt</span> <span class="pre">-t</span> <span class="pre">002c_claudia_and_the_phantom_phone_calls-s.txt</span> <span class="pre">--srctotarget</span> <span class="pre">fr002_bg-mt.txt</span> <span class="pre">-o</span> <span class="pre">fr002_bg-aligned</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">./bleualign.py</span> <span class="pre">-s</span> <span class="pre">fr003_bg-s.txt</span> <span class="pre">-t</span> <span class="pre">003c_the_truth_about_stacey-s.txt</span> <span class="pre">--srctotarget</span> <span class="pre">fr003_bg-mt.txt</span> <span class="pre">-o</span> <span class="pre">fr003_bg-aligned</span></code></p>
<p>And to get the commands for doing the same alignment for the Quebec translations, I could use find-and-replace in my text file to substitute “_bg” with “_qu” everywhere.</p>
<p>Each time you run one of those commands, you get a pair of output files whose filename matches your output name, with one letter added onto the end. For me, <em>outputname-t</em> was English sentences, and <em>outputname-s</em> was French sentences. (You should also rename all the output files to add .txt, too.) In theory, if the alignment worked, sentence 1 in <em>outputname-s</em> should be a translation of sentence 1 in <em>outputname-t</em>.</p>
<p>But there’s a catch: Bleualign doesn’t like to admit when it’s confused or ask for help. If, for some reason, you’ve made a typo in the machine translated text filename (or if you forget to put the machine translation in the folder before you run it), Bleualign will happily go about “aligning” your text based on nothing but its own creativity. Which, let me tell you, is very creative:</p>
<p><img alt="Tweets from Quinn about a complete failure to align a sentence about potroast" src="_images/dscm4_potroast.png" /></p>
<p>So if your results are truly awful, make sure you got all your filenames right.</p>
</div>
</div>
<div class="section" id="comparing-just-the-food">
<h3>Comparing just the food<a class="headerlink" href="#comparing-just-the-food" title="Permalink to this headline">¶</a></h3>
<p>Finally, we could put all the pieces together! I put the output files (the -s and -t ones from Bleualign) into three folders based on the country of origin for the translation, and wrote up some Python code that would look through each English sentence for the food words from the list I’d created with help from WordNet, and if it found one of those words, it’d take the English sentence and the translation and write them to a new CSV file that we’d use as our spreadsheet.</p>
<div class="section" id="specifying-the-directory-and-language">
<h4>Specifying the directory and language<a class="headerlink" href="#specifying-the-directory-and-language" title="Permalink to this headline">¶</a></h4>
<p>Put in the full path to the directory that has just the Bleualign output (all the outputname-s.txt and outputname-t.txt files), and also put in the name of the translation language. I was already hoping we could use this code for other translations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Full path to the directory</span>
<span class="n">directory</span> <span class="o">=</span> <span class="s1">&#39;/Users/qad/Documents/dsc_french/french_aligned&#39;</span>
<span class="c1">#Changes to that directory</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span>
<span class="c1">#Name of the language, lowercase because it&#39;s going in a filename</span>
<span class="n">lang</span> <span class="o">=</span> <span class="s1">&#39;french&#39;</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="specifying-the-word-list">
<h4>Specifying the word list<a class="headerlink" href="#specifying-the-word-list" title="Permalink to this headline">¶</a></h4>
<p>Remember that clean list of food words in the original English texts that we got from WordNet earlier in this book? It’s time to put it to use! In the next step, we put in the full path to that file, and read it into a Python list.</p>
<p>You may notice the spaces around the words; this is to make sure that we don’t get non-food words like <em>appear</em> just because they have <em>pear</em> in them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Open the file with all the food terms</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;/Users/qad/Documents/dsc-food-terms.csv&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">foodfile</span><span class="p">:</span>
    <span class="c1">#Read it into a list called foods</span>
    <span class="n">foods</span> <span class="o">=</span> <span class="n">foodfile</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
    <span class="c1">#Print the list to make sure we&#39;re getting what we want</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">foods</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="identifying-sentence-pairs-with-food-words">
<h4>Identifying sentence pairs with food words<a class="headerlink" href="#identifying-sentence-pairs-with-food-words" title="Permalink to this headline">¶</a></h4>
<p>Here we go! This code opens the English original text and its (theoretically aligned) translation, searches for all the foods words in the food list in each of the English sentences, and if there’s a match, writes out that English sentence and its (hopefully) corresponding translation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Creates an output file that follows the pattern:</span>
<span class="c1">#dsc - languagename - food.csv</span>
<span class="n">outname</span> <span class="o">=</span> <span class="s1">&#39;dsc-&#39;</span> <span class="o">+</span> <span class="n">lang</span> <span class="o">+</span> <span class="s1">&#39;-food.csv&#39;</span>
<span class="c1">#Opens the output file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">outname</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">out</span><span class="p">:</span>
    <span class="c1">#Writes a header row with the filename, english, then the language</span>
    <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;file, english, &#39;</span> <span class="o">+</span> <span class="n">lang</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1">#For each file in the directory of Bleualign outputs you specified</span>
    <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
        <span class="c1">#If the filename ends with -t...</span>
        <span class="c1">#(This means the English file)</span>
        <span class="k">if</span> <span class="n">filename</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;-t.txt&#39;</span><span class="p">):</span>
            <span class="c1">#The name of the file with the translation is the same</span>
            <span class="c1">#Except instead of -t it&#39;s -s</span>
            <span class="n">langname</span> <span class="o">=</span> <span class="n">filename</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;-t.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;-s.txt&#39;</span><span class="p">)</span>
            <span class="c1">#Open each filename that ends with -t as enf</span>
            <span class="c1">#enf for &quot;English file&quot;, I just made that up</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">enf</span><span class="p">:</span>
                <span class="c1">#Open each filename that ends with -s as langf</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">langname</span><span class="p">)</span> <span class="k">as</span> <span class="n">langf</span><span class="p">:</span>
                    <span class="c1">#Split the English text into a list of sentences</span>
                    <span class="n">enlines</span> <span class="o">=</span> <span class="n">enf</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
                    <span class="c1">#Split the translated text into a list of sentences</span>
                    <span class="n">langlines</span> <span class="o">=</span> <span class="n">langf</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span> 
                    <span class="c1">#For each sentence in the list of English sentences</span>
                    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">enlines</span><span class="p">:</span>
                        <span class="c1">#Grab what English sentence number it is</span>
                        <span class="c1">#This is important to link up with</span>
                        <span class="c1">#the aligned translation later</span>
                        <span class="n">linenum</span> <span class="o">=</span> <span class="n">enlines</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                        <span class="c1">#If there&#39;s a match for any of the food words</span>
                        <span class="c1">#in the English sentence</span>
                        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">food</span> <span class="ow">in</span> <span class="n">line</span> <span class="k">for</span> <span class="n">food</span> <span class="ow">in</span> <span class="n">foods</span><span class="p">):</span>
                            <span class="c1">#Remove any commas from the English sentence</span>
                            <span class="c1">#Commas can mess up our CSV output</span>
                            <span class="n">cleanline</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
                            <span class="c1">#Remove any quotation marks from the English sentence</span>
                            <span class="c1">#Quotation marks can mess up our CSV output</span>
                            <span class="n">cleanline</span> <span class="o">=</span> <span class="n">cleanline</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;&quot;&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
                            <span class="c1">#Remove any commas from the corresponding translation</span>
                            <span class="n">cleanlang</span> <span class="o">=</span> <span class="n">langlines</span><span class="p">[</span><span class="n">linenum</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
                            <span class="c1">#Remove any quotation marks from the corresponding translation</span>
                            <span class="n">cleanlang</span> <span class="o">=</span> <span class="n">cleanlang</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;&quot;&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
                            <span class="c1">#Write the filename, English, and translation lines and a newline</span>
                            <span class="n">out</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">langname</span> <span class="o">+</span> <span class="s1">&#39;, &#39;</span> <span class="o">+</span> <span class="n">cleanline</span> <span class="o">+</span> <span class="s1">&#39;, &#39;</span> <span class="o">+</span> <span class="n">cleanlang</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>I uploaded the files to Google Drive, and sent a note off to Isabelle and Lee to tell them I had data!</p>
</div>
</div>
</div>
<div class="section" id="id2">
<h2>Isabelle<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>Quinn’s brilliant work made it really easy to get an overview of the different strategies used to translate food items. The first discovery from the Belgian books was that these strategies were not consistently used across the books, or even within the books themselves. Branded foods were either left as in the original, turned into local brands, or translated into a generic, unbranded food name. In this slide from our CSDH-SCHN presentation, we can see how Twinkies, Claudia’s staple snack, becomes very versatile in the Belgian edition:</p>
<p><img alt="Inconsistencies in Twinkie translation" src="_images/dscm4_inconsistencies.png" /></p>
<p>Unbranded food items were often transformed to fit local preferences. A fudgesicle becomes a chocolate fondant and the PB&amp;J sandwich is safely divided into two separate sandwiches.</p>
<p><img alt="Localizing food preferences" src="_images/dscm4_localize_preferences.png" /></p>
<p>When local brands were adopted, it could either be a brand which resembled the original product by look, taste or even occasionally sound (as shown in the slide below).</p>
<p><img alt="Localizing food preferences using sound" src="_images/dscm4_similar_name.png" /></p>
<p>Other times, the translator had understandably ran out of steam for dealing with these foods and opted to do without:</p>
<p><img alt="Omitting foods" src="_images/dscm4_omissions.png" /></p>
<p>When we compared food names in the Belgian and French versions, we discovered that the translations themselves were often nearly identical, with whole paragraphs reproduced word for word. However, maybe due to increased exposure to US culture in Europe, the US American flavor of the series was reintroduced and a few food items from the original, such as peanut butter, made it to the French version, alongside some of the original names and cultural references. This evolution created a cool hybrid food landscape, where items which were readable for a European audience were rescued from the original text, while others remained unapologetically French.</p>
<p>All in all, this food exploration led us to expected and unexpected discoveries. It became very clear that food was a key aspect of the BSC series and one which likely contributed to its appeal for many kids salivating at Claudia’s endless string of hidden snacks. Between Stacey’s diabetes, the portrayal of suburban family life at meal time and kids’ general infatuation with junk food, food is woven in the narrative at every turn. Branded items in particular contribute to the ‘colorful’ vibe of the series while simultaneously providing a distinguishable US American ‘local color’. The translators’ choices regarding food were therefore important and revealing about the evolving attitude of the children publishing industry towards US culture. The US Americanness of the baby-sitters’ food choices needed to be overwritten in the earlier Belgian version, but likely became part of the allure of the series in the later French editions. The food choices of the translators also gave us insights on the translation process. Whilst Doritos became Bonitos in the Belgian edition (likely due a similar sonority), they became M&amp;Ms in the subsequent French edition. Since Bonitos are French ancestors of M&amp;Ms, this translation choice suggests that the translator, in this instance, didn’t go back to the original version but rather ‘updated’ the French version to reflect current food preferences and availability.</p>
</div>
<div class="section" id="id3">
<h2>Quinn<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>Something was bugging me as I looked through the data, and Isabelle’s observations about the relationship between the Belgian and French food translations helped the pieces fall into place. Our CSDH-SCHN presentation video was due a week before the conference – before we’d finished with the data analysis – so we just talked through Isabelle’s wonderful examples of different adaptation strategies, and concluded with a promissory note for a quantitative analysis as part of our synchronous discussion.</p>
<p>In the last days before our CSDH-SCHN synchronous presentation, I churned through the data in the spreadsheet I’d generated, going through all four translations (Quebec, Belgian, original French, and re-released French) for each book before moving on to the next. And I started to notice things. We were sure that the Quebec translations took place in isolation from other French translations, and as part of our <a class="reference external" href="https://docs.google.com/presentation/d/15SZjrQis3Xd3N3AEtaWuatIR-PeM6cAnD1H5oBbd_NQ/edit#slide=id.p">presentation for virtual Flyover Comics</a>, we’d discovered that the recent book re-releases in France had been edited into a more colloquial modern French. But all the character and place names were localized in the Belgian version, and unadapted in the French translation (with a few exceptions, like StaceyMcGill becoming Lucy MacDouglas, and Watson Brewer becoming Jim Lelland), so we figured these had to be different translations. Frustratingly, though, the Belgians were not good about documenting the translator; there was no translator credited in the books themselves, or in the library records, for the majority of our Belgian translations. We harbored secret dreams of trying to triangulate stylometry vs. original English ghostwriters to try to identify at least how many Belgian translators there were, but that would be a much-later DSC Multilingual Mystery, we thought.</p>
<p>But it turned out we didn’t need to do that after all.</p>
<p>Because as I read through the same English sentences, in these presumably “different” French and Belgian translations, I realized something: they were eerily similar. Character and place names were different, sure, and there were also differences in the food translation strategies, but the phrasing was often identical.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>English</p></th>
<th class="head"><p>Belgian</p></th>
<th class="head"><p>French</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>First I’ll eat dinner with you and then I’ll have dessert in the triplets’ room – but I’ll bring the TV in here.</p></td>
<td><p>Je mangerai avec vous et je prendrai le dessert chez les triplés mais je vous apporterai la télé.</p></td>
<td><p>Je mangerai avec vous et je prendrai le dessert dans la chambre des triplés mais je vous apporterai la télé.</p></td>
</tr>
<tr class="row-odd"><td><p>She and Dawn would probably get along great since Dawn only eats stuff like fruit and granola and vegetables.</p></td>
<td><p>Elle et Carole s’entendraient probablement à merveille puisque Carole ne mange que des fruits des légumes ou des céréales.</p></td>
<td><p>Elle et Carla s’entendraient probablement à merveille puisque Carla ne mange que des fruits des légumes ou des céréales.</p></td>
</tr>
<tr class="row-even"><td><p>She set out a bowl of broccoli, a fresh green salad, and a plate of baked chicken legs, all foods I can eat.</p></td>
<td><p>Elle apporta un saladier de brocolis une salade verte et une assiette de cuisses de poulet rôties c’est-à-dire ce que je pouvais manger.</p></td>
<td><p>Elle apporta un bol de brocolis de la salade verte et une assiette de cuisses de poulet rôties c’est-à-dire ce que je pouvais manger.</p></td>
</tr>
</tbody>
</table>
<p><em>BSC #19: Claudia and the Bad Joke</em> was the book that brought it all into focus. The Belgian version, <em>Une mauvaise farce pour Julie</em>, was one of the few that credited the translator, who happened to be named Françoise Rose. The French version, <em>La revanche de Carla</em>, was credited to Françoise Rose… and Camille Weil. It was an overlap we hadn’t noticed before, not even when Lee downloaded the metadata for these books from national library records in <a class="reference external" href="https://datasittersclub.github.io/site/dscm1.html#lee">DSC Multilingual Mystery 1: Lee and the Missing Metadata</a>. I started digging. And, indeed, the French translations always had Camille Weil along with some other name, up until the point in the series where the Belgian translations stopped.</p>
<p>Completely by accident, we’d found our Belgian translators, and we didn’t even have to try any fancy computational text analysis methods to do it.</p>
<p>What’s more, tagging the food examples with the translation strategy used, and visualizing the results, helped reveal the handiwork of Camile Weil through the ways the French translations differed from the Belgian translations they were based on:</p>
<p><img alt="Pie charts showing food translation strategies, as they differ between Belgian and French books" src="_images/dscm4_camille_weil.png" /></p>
<p>While most of her work on the series involved adapting the Belgian books, she did translate a few books on her own, like <em>#29: Mallory et la malle mystérieuse</em>, which I immediately ordered. I can’t wait to see how her food-translation choices align (or don’t) with how she went about changing the Belgian translations!</p>
<p>But that’s a story for another book.</p>
</div>
<div class="section" id="id4">
<h2>Lee<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p><img alt="Look, I’ll be honest - for this book, I’m the data-sitter who chimes in occasionally but doesn’t play a central role in the particular narrative. This is the hand-written entry from one of the other sitters that breaks up the narrative a little bit to remind you that others exist outside of the main narrator." src="_images/dscm4_leehandwritten.png" /></p>
<p>I wish I had had the time to do the same kind of work that Isabelle did but with the Québécois translations. And maybe (hopefully?) I will have time and we can have a rich discussion around food and culture and Americanness and Québécois and Belgium and the 1980s and 1990s. But for now, we have these wonderful insights and visualizations. The translator in me is all excited because of things like translating food brand-names according to sound. Who knew that a translator of a 1980s YA series for girls would take the time and care to think about the sonic effects?</p>
<p>But I also wonder about, well, how did they translate “Twinky” in Quebec, another brand we were aware of but literally had no access to in Canada (save for cross-border shopping). But, what we did have access to were our own local Vachon snack-cakes (which I know entirely too much about because of another writing project I’m doing). So, why not translate Twinkie for, say, a Jos Louis or a May West? Really lean into the localization!</p>
<p>So I still have all these questions that Isabelle helped me to formulate through her work with the Belgian translations. And I’m excited about them! This is great! I love the possibilities. And I think the more we did into these questions, the more it could tell us about the reception of these books outside of the USA, and how this “Americanness” was viewed or received. And what can those translations tell us about the original books, things that we may have taken for granted? Translation is a fascinating lens to look at both cultures in question, to bring different elements into focus.</p>
<p>Because sometimes a Twinky isn’t just a Twinky, ya know? ;-)</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The Data-Sitters Club<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>